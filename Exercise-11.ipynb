{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits dimensionality reduction with scikit-learn\n",
    "\n",
    "In this notebook, we'll use some popular methods to reduce the dimensionality of MNIST digits data before classification.\n",
    "\n",
    "[Section 1](#1.-Feature-extraction) of the notebook contains examples of feature extraction methods, and [Section 2](#2.-Feature-selection) two  methods for feature selection. Any of these methods can then be applied to train a MNIST digits classifier for lower-dimensional data in [Section 3](#3.-Classification-with-dimension-reduced-data).\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pml_utils import get_mnist\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import __version__\n",
    "from sklearn import decomposition, feature_selection\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.feature import canny\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from packaging.version import Version\n",
    "assert(Version(__version__) >= Version(\"0.20\")), \"Version >= 0.20 of sklearn is required.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the MNIST data. First time it may download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = get_mnist('MNIST')\n",
    "\n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature extraction\n",
    "\n",
    "### 1.1 PCA\n",
    "\n",
    "[Principal component analysis](http://scikit-learn.org/stable/modules/decomposition.html#pca) (PCA) is a standard method to decompose a high-dimensional dataset in a set of successive orthogonal components that explain a maximum amount of the variance. Here we project the data into `n_components` principal components. The components have the maximal possible variance under the orthogonality constraint.\n",
    "\n",
    "The option `whiten=True` can be used to whiten the outputs to have unit component-wise variances.  Its usefulness depends on the model to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 50\n",
    "pca = decomposition.PCA(n_components=n_components, whiten=True)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "print('X_pca:', X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the amount of variance explained by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(n_components)+1, pca.explained_variance_)\n",
    "plt.title('Explained variance by PCA components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.xlabel('PCA component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Image feature extraction\n",
    "\n",
    "There are a lot of different feature extraction methods for image data.  Common ones include extraction of colors, textures, and shapes from images, or detection of edges, corners, lines, blobs, or templates.  Let's try a simple filtering-based method to reduce the dimensionality of the features, and a widely-used edge detector.\n",
    "\n",
    "The [`measure.block_reduce()`](http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce) function from scikit-image applies a function (for_example `np.mean`, `np.max` or `np.median`) to blocks of the image, resulting in a downsampled image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_img = X_train.reshape(-1, 28, 28)\n",
    "filter_size = 2\n",
    "X_train_img_downsampled = block_reduce(X_train_img, \n",
    "                                       block_size=(1, filter_size, filter_size), \n",
    "                                       func=np.mean)\n",
    "\n",
    "print('X_train_img:', X_train_img.shape)\n",
    "print('X_train_img_downsampled:', X_train_img_downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`feature.canny()`](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.canny) function applies the [Canny edge detector](https://en.wikipedia.org/wiki/Canny_edge_detector) to extract edges from the image.  Processing all images may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sigma = 1.0\n",
    "X_train_img_canny = np.zeros(X_train_img.shape)\n",
    "for i in range(X_train_img.shape[0]):\n",
    "    X_train_img_canny[i,:,:] = canny(X_train_img[i,:,:], sigma=sigma)\n",
    "print('X_train_img_canny:', X_train_img_canny.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the original and filtered digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Original')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_img[i,:,:], cmap=\"gray\", interpolation='none')\n",
    "\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Downsampled with a %dx%d filter' % (filter_size, filter_size))\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_img_downsampled[i,:,:], cmap=\"gray\", interpolation='none')\n",
    "    \n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "plt.suptitle('Canny edge detection with sigma=%.2f' % sigma)\n",
    "plt.subplots_adjust(top=0.8)\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train_img_canny[i,:,:], cmap=\"gray\", interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n",
    "\n",
    "### 2.1 Low variance\n",
    "\n",
    "The MNIST digits have a lot of components (pixels) with little variance.  These components are not particularly useful for discriminating between the classes, so they can probably be removed safely.  Let's first draw the component-wise variances of MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "variances = np.var(X_train, axis=0)\n",
    "plt.figure()\n",
    "plt.plot(variances)\n",
    "plt.title('Component-wise variance of MNIST digits')\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances can also be plotted for each pixel in the image plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(variances.reshape(28,28), cmap=sns.color_palette(\"Blues\"))\n",
    "plt.title('Pixel-wise variance of MNIST digits')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an appropriate `variance_threshold` based on the *\"Component-wise variance of MNIST digits\"* figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "variance_threshold = 1000\n",
    "lv = feature_selection.VarianceThreshold(threshold=variance_threshold)\n",
    "X_lv = lv.fit_transform(X_train)\n",
    "print('X_lv:', X_lv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Univariate feature selection\n",
    "\n",
    "Another method for feature selection is to select the *k* best features based on univariate statistical tests between the features and the class of each sample.  Therefore, this is a supervised method and we need to include `y_train` in `fit_transform()`.\n",
    "See [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) for the set of available statistical tests and other further options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k = 50\n",
    "ukb = feature_selection.SelectKBest(k=k)\n",
    "X_ukb = ukb.fit_transform(X_train, y_train)\n",
    "print('X_ukb:', X_ukb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which features (that is, pixels in case) got selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "support = ukb.get_support()\n",
    "plt.figure()\n",
    "sns.heatmap(support.reshape(28,28), cmap=sns.color_palette(\"Blues\"))\n",
    "#with sns.axes_style(\"white\"):\n",
    "#    plt.imshow(support.reshape(28,28), interpolation='none')\n",
    "plt.title('Support of SelectKBest() with k=%d' % k)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification with dimension-reduced data \n",
    "\n",
    "Let's now train a classifier using lower-dimensional data. Choose any of the above feature extraction or feature selection methods, and reduce the dimensionality of the MNIST data with that method. You can also implement your own dimensionaly reduction method.\n",
    "\n",
    "Note that you need to transform also the test data into the lower-dimensional space using `transform()`.  Here is an example for PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(X_test)\n",
    "print('X_test_pca:', X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a classification method from the ones that have been discussed on the previous lectures. For example, nearest neighbor classifiers or decision trees are good choices. Compare the results (accuracy, time) to classification using the original MNIST data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other methods for dimensionality reduction\n",
    "\n",
    "Study and experiment with additional dimensionality reduction methods based on [decomposing](http://scikit-learn.org/stable/modules/decomposition.html) or [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html).  See also [unsupervised dimensionality reduction](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
